<!-- projects/templates/projects/blacklight.html -->
{% extends 'base.html' %}

{% block content %}
<section class="hero" style="background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%); padding: 80px 0;">
    <div class="container">
        <h1 style="font-size: 3rem; margin-bottom: 20px;">Project Blacklight</h1>
        <p style="font-size: 1.3rem; margin-bottom: 40px; max-width: 600px; margin-left: auto; margin-right: auto;">
            Revealing true optimizer performance against known global minima
        </p>
        <div class="project-status" style="background: #e74c3c; margin-bottom: 20px;">Research Phase</div>
    </div>
</section>

<section class="content-section">
    <div class="container">
        <div class="about-content">
            <h2>The Hidden Problem</h2>
            <p>Machine learning optimizers make bold claims about performance, but how do we know how close they actually get to the true global minimum? In practice, we never know where the global minimum is, so optimizer comparisons rely on relative performance - which can be misleading.</p>

            <div class="collaboration-highlight">
                <h3>The Blacklight Metaphor</h3>
                <p>Like a blacklight revealing hidden stains invisible under normal light, Project Blacklight exposes what optimizers actually do in the dark by testing them against problems where we <em>do</em> know the true answer.</p>
            </div>

            <h2>Our Approach</h2>
            <p>Project Blacklight generates toy neural networks small enough that we can find the global minimum through exhaustive search or mathematical analysis. We then test various optimizers (SGD, Adam, RMSprop, BFGS, etc.) with statistical rigor to see how close they actually get.</p>

            <h2>Methodology</h2>
            <p>Our research protocol includes:</p>
            <ul style="margin: 20px 0; padding-left: 30px; line-height: 1.8;">
                <li><strong>Toy Problem Generation</strong>: Create neural networks small enough for exact solutions</li>
                <li><strong>Statistical Rigor</strong>: Minimum 20-50 runs per optimizer configuration</li>
                <li><strong>Comprehensive Analysis</strong>: p-values, confidence intervals, quartiles, variance measurements</li>
                <li><strong>Multiple Architectures</strong>: Test across different network topologies and activation functions</li>
                <li><strong>Hyperparameter Sensitivity</strong>: Systematic exploration of learning rates and other settings</li>
            </ul>

            <h2>Expected Insights</h2>
            <p>This research will reveal:</p>
            <ul style="margin: 20px 0; padding-left: 30px; line-height: 1.8;">
                <li><strong>Optimizer Gaps</strong>: How far optimizers typically fall short of true optima</li>
                <li><strong>Consistency Patterns</strong>: Which optimizers are most reliable across problems</li>
                <li><strong>Hyperparameter Sensitivity</strong>: How robust optimizers are to parameter choices</li>
                <li><strong>Problem-Specific Performance</strong>: Which optimizers work best for different loss landscapes</li>
            </ul>

            <h2>Impact</h2>
            <p>Project Blacklight will provide the machine learning community with:</p>
            <ul style="margin: 20px 0; padding-left: 30px; line-height: 1.8;">
                <li><strong>Evidence-Based Optimizer Selection</strong>: Move beyond marketing claims to actual performance data</li>
                <li><strong>Realistic Expectations</strong>: Understand the true limitations of current optimization methods</li>
                <li><strong>Research Direction</strong>: Identify where optimizer improvements are most needed</li>
                <li><strong>Practical Guidelines</strong>: Provide actionable advice for practitioners</li>
            </ul>

            <h2>Technical Infrastructure</h2>
            <div class="collaboration-highlight">
                <p><strong>Development Platform</strong>: Forge system with RTX 5070 Ti (16GB VRAM)<br>
                <strong>Computational Requirements</strong>: Embarrassingly parallel - perfect for GPU acceleration<br>
                <strong>Timeline</strong>: Research phase active, implementation planned for 2025</p>
            </div>

            <h2>Research Questions</h2>
            <p>Key questions we aim to answer:</p>
            <ul style="margin: 20px 0; padding-left: 30px; line-height: 1.8;">
                <li>How much optimization performance do we actually lose by not finding global minima?</li>
                <li>Are newer optimizers like AdamW meaningfully better than SGD on problems we can solve exactly?</li>
                <li>Do optimizer performance patterns on toy problems predict real-world behavior?</li>
                <li>What loss landscape characteristics make certain optimizers succeed or fail?</li>
            </ul>

            <div style="text-align: center; margin: 40px 0;">
                <a href="mailto:contact@sgcx.org?subject=Project%20Blacklight%20Collaboration" class="cta-button" style="margin-right: 15px;">Collaborate with Us</a>
                <a href="{% url 'projects:project_list' %}" class="project-link">‚Üê All Projects</a>
            </div>
        </div>
    </div>
</section>
{% endblock %}